{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('../kitti_eval/flow_tool/')\n",
    "sys.path.append('..')\n",
    "import flowlib as fl\n",
    "\n",
    "from nets.depth_net import Depth_net\n",
    "from nets.pose_net import Pose_net\n",
    "from nets.flow_net import Flow_net\n",
    "\n",
    "from utils.optical_flow_warp_fwd import transformerFwd\n",
    "from utils.optical_flow_warp_old import transformer_old\n",
    "from utils.loss_utils import SSIM, cal_grad2_error_mask, charbonnier_loss, cal_grad2_error, compute_edge_aware_smooth_loss, ternary_loss, depth_smoothness\n",
    "from utils.utils import average_gradients, normalize_depth_for_display, preprocess_image, deprocess_image, inverse_warp, inverse_warp_new\n",
    "\n",
    "from data_loader.data_loader import DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import imageio\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0:2], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mode\n",
    "# mode = \"train_flow\"\n",
    "mode = \"train_dp\"\n",
    "\n",
    "#是否要載入 pretrain weights\n",
    "LOAD_FLOW_WEIGHT = False\n",
    "LOAD_DP_WEIGHT = False\n",
    "\n",
    "if mode == 'train_dp':\n",
    "    LOAD_FLOW_WEIGHT = True\n",
    "\n",
    "# Dataset路徑\n",
    "kitti_depth_folder = '../../../../datasets/kitti_3frames_256_832'\n",
    "\n",
    "\n",
    "# Model Input 解析度設定\n",
    "EPOCH = 50\n",
    "BATCH_SIZE = 8\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 832\n",
    "NUM_SCALE = 4\n",
    "NUM_SOURCE = 2\n",
    "\n",
    "# Loss Hyperparameters\n",
    "SSIM_WEIGHT = 0.85\n",
    "FLOW_RECONSTRUCTION_WEIGHT = 1.0\n",
    "FLOW_SMOOTH_WEIGHT = 10.0\n",
    "FLOW_CROSS_GEOMETRY_WEIGHT = 0.3\n",
    "FLOW_DIFF_THRESHOLD = 4.0\n",
    "FLOW_CONSIST_WEIGHT = 0.01\n",
    "\n",
    "DP_RECONSTRUCTION_WEIGHT = 50.0\n",
    "DP_SMOOTH_WEIGHT = 10.0 # origin 1.0\n",
    "DP_CROSS_GEOMETRY_WEIGHT = 0.8\n",
    "\n",
    "compute_minimum_loss = False\n",
    "is_depth_upsampling = False # [!!!!] Cannot work. => cause depth smoothness loss to be 0\n",
    "joint_encoder = False\n",
    "equal_weighting = False  # equal weight for depth smoothness loss\n",
    "depth_normalization = False # depth normalization for depth smoothness loss\n",
    "scale_normalize = True # depth normalization for all loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(mode=mode, \n",
    "                        dataset_dir=kitti_depth_folder, \n",
    "                        img_height=IMG_HEIGHT, \n",
    "                        img_width=IMG_WIDTH,\n",
    "                        split='train')\n",
    "dataset = dataloader.build_dataloader().batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_model = Flow_net(input_shape=[IMG_HEIGHT, IMG_WIDTH, 9])\n",
    "flow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'shufflenetv2', 'mobilenetv2', 'mnasnet', 'mobilenetv3'\n",
    "net_name = 'shufflenetv2'\n",
    "depth_model = Depth_net(net_name, input_shape=[IMG_HEIGHT,IMG_WIDTH,3], training=True)\n",
    "depth_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model = Pose_net(input_shape=[IMG_HEIGHT, IMG_WIDTH, 9])\n",
    "pose_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Optimizers and Checkpoint-saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-4\n",
    "flow_optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "dp_optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_attr = str(BATCH_SIZE) + '_' + str(IMG_HEIGHT) + '_' + str(IMG_WIDTH)\n",
    "checkpoint_dir_flow = os.path.join('../training_checkpoints', 'train_flow', checkpoint_attr)\n",
    "checkpoint_dir_dp = os.path.join('../training_checkpoints', 'train_dp', checkpoint_attr)\n",
    "if not os.path.isdir(checkpoint_dir_flow):\n",
    "    os.makedirs(checkpoint_dir_flow)\n",
    "if not os.path.isdir(checkpoint_dir_dp):\n",
    "    os.makedirs(checkpoint_dir_dp)\n",
    "    \n",
    "checkpoint_prefix_flow = os.path.join(checkpoint_dir_flow, \"ckpt\")\n",
    "checkpoint_prefix_dp = os.path.join(checkpoint_dir_dp, \"ckpt\")\n",
    "\n",
    "checkpoint_flow = tf.train.Checkpoint(flowModel=flow_model, \n",
    "                                      flowOptimizer=flow_optimizer)\n",
    "checkpoint_dp = tf.train.Checkpoint(depthModel=depth_model,\n",
    "                                    poseModel=pose_model,\n",
    "                                    dpOptimizer=dp_optimizer)\n",
    "\n",
    "if LOAD_FLOW_WEIGHT:\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "if LOAD_DP_WEIGHT:\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset, epochs):\n",
    "    his_loss = []\n",
    "    for epoch in range(EPOCH):\n",
    "        total_loss = 0\n",
    "        count = 1\n",
    "        start = time.time()\n",
    "        for image_batch, cam in dataset:\n",
    "\n",
    "            loss = train_step(image_batch, cam)\n",
    "            total_loss += loss.numpy()\n",
    "            if count % 20 == 0:\n",
    "                print('[info] Epoch:{} Count:{} Loss:{} Time:{:.2f} sec'.format(epoch, count, total_loss/count, time.time()-start))\n",
    "                show_image(image_batch[0], flow[0], occlusion[0], epoch)\n",
    "                if count % 200 == 0:\n",
    "                    clear_output(wait=True)\n",
    "                    print('[info] Epoch:{} Count:{} Loss:{} Time:{:.2f} sec'.format(epoch, count, total_loss/count, time.time()-start))\n",
    "                    show_image(image_batch[0], flow[0], occlusion[0], epoch, isSave=True)\n",
    "                start = time.time()\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        his_loss.append(total_loss/count)\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "        \n",
    "\n",
    "    return his_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, cam):\n",
    "    predict_flow = flow_model(images)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predict_disp_src0 = depth_model(images[:, :, :, :3]) #training=True\n",
    "        predict_disp_tgt = depth_model(images[:, :, :, 3:6]) #training=True\n",
    "        predict_disp_src1 = depth_model(images[:, :, :, 6:]) #training=True\n",
    "        predict_pose = pose_model(images)\n",
    "        loss = build_dp_loss(images, \n",
    "                             [predict_disp_src0, predict_disp_tgt, predict_disp_src1], \n",
    "                             predict_pose, \n",
    "                             predict_flow,\n",
    "                             cam)\n",
    "\n",
    "    gradients_of_depth = tape.gradient(loss, depth_model.trainable_variables)\n",
    "    gradients_of_pose = tape.gradient(loss, pose_model.trainable_variables)\n",
    "    dp_optimizer.apply_gradients(zip(gradients_of_depth, depth_model.trainable_variables))\n",
    "    dp_optimizer.apply_gradients(zip(gradients_of_pose, pose_model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def build_dp_loss(images, predict_disp, predict_pose, predict_flow, cam):\n",
    "    disp = {}\n",
    "    depth = {}\n",
    "#     depth_upsampled = {}\n",
    "    \n",
    "    tgt_image = input_images[:, :, :, 3:6]\n",
    "    src_image_stack = tf.concat([input_images[:, :, :, :3], input_images[:, :, :, 6:]], axis=3)\n",
    "    image_name = ['src0', 'tgt', 'src1']\n",
    "    for i, name in enumerate(image_name):\n",
    "        if scale_normalize:\n",
    "            # As proposed in https://arxiv.org/abs/1712.00175, this can\n",
    "            # bring improvement in depth estimation, but not included in our paper.\n",
    "            predict_disp[i] = [spatial_normalize(d) for d in predict_disp[i]]\n",
    "\n",
    "        predict_depth = [1. / d for d in predict_disp[i]]\n",
    "        \n",
    "        disp[name] = predict_disp[i]\n",
    "        depth[name] = predict_depth\n",
    "#         depth_upsampled[name] = [tf.image.resize(d, [IMG_HEIGHT, IMG_WIDTH],\n",
    "#                                  method=tf.image.ResizeMethod.BILINEAR) \n",
    "#                                  for d in predict_depth]\n",
    "        \n",
    "    \n",
    "    pred_fw_flows = predict_flow[:3]\n",
    "    pred_bw_flows = predict_flow[3:]\n",
    "    \n",
    "    proj_cam2pix = cam\n",
    "    proj_pix2cam = tf.linalg.inv(cam)\n",
    "    \n",
    "    smooth_loss = 0\n",
    "    reconstructed_loss = 0\n",
    "    cross_reconstructed_loss = 0\n",
    "    ssim_loss = 0\n",
    "    cross_ssim_loss = 0\n",
    "\n",
    "    proj_error_depth_all = []\n",
    "    flyout_map_all_tgt = []\n",
    "    flyout_map_all_src0 = []\n",
    "    flyout_map_all_src1 = []\n",
    "    curr_tgt_image_all = []\n",
    "    curr_src_image_stack_all = []\n",
    "    proj_error_src0 = []\n",
    "    proj_error_src0_1 = []\n",
    "    proj_error_src1 = []\n",
    "    proj_error_src1_1 = []\n",
    "    proj_error_tgt = []\n",
    "    proj_error_tgt1 = []\n",
    "    upsampled_tgt_depth_all = []\n",
    "    summaries = []\n",
    "    \n",
    "    # Calculate different scale occulsion maps described in 'Occlusion Aware Unsupervised\n",
    "    # Learning of Optical Flow by Yang Wang et al'\n",
    "    occu_masks_bw = []\n",
    "    occu_masks_bw_avg = []\n",
    "    occu_masks_fw = []\n",
    "    occu_masks_fw_avg = []\n",
    "\n",
    "    for i in range(len(pred_bw_flows)):\n",
    "        temp_occu_masks_bw = []\n",
    "        temp_occu_masks_bw_avg = []\n",
    "        temp_occu_masks_fw = []\n",
    "        temp_occu_masks_fw_avg = []\n",
    "\n",
    "        for s in range(NUM_SCALE):\n",
    "            H = int(IMG_HEIGHT / (2**s))\n",
    "            W = int(IMG_WIDTH  / (2**s))\n",
    "\n",
    "            mask, mask_avg = occulsion(pred_bw_flows[i][s], H, W)\n",
    "            temp_occu_masks_bw.append(mask)\n",
    "            temp_occu_masks_bw_avg.append(mask_avg)\n",
    "            # [src0, tgt, src0_1]\n",
    "\n",
    "            mask, mask_avg = occulsion(pred_fw_flows[i][s], H, W)\n",
    "            temp_occu_masks_fw.append(mask)\n",
    "            temp_occu_masks_fw_avg.append(mask_avg)\n",
    "            # [tgt, src1, src1_1]\n",
    "\n",
    "        occu_masks_bw.append(temp_occu_masks_bw)\n",
    "        occu_masks_bw_avg.append(temp_occu_masks_bw_avg)\n",
    "        occu_masks_fw.append(temp_occu_masks_fw)\n",
    "        occu_masks_fw_avg.append(temp_occu_masks_fw_avg)\n",
    "    \n",
    "    \n",
    "    scaled_tgt_images = [None for _ in range(NUM_SCALE)]\n",
    "    scaled_src_images_stack = [None for _ in range(NUM_SCALE)]\n",
    "    for s in range(NUM_SCALE):\n",
    "        H = int(IMG_HEIGHT / (2**s))\n",
    "        W = int(IMG_WIDTH  / (2**s))\n",
    "        if s == 0: # Just as a precaution. TF often has interpolation bugs.\n",
    "            scaled_tgt_images[s] = tgt_image\n",
    "            scaled_src_images_stack[s] = src_image_stack\n",
    "        else:\n",
    "            scaled_tgt_images[s] = tf.image.resize(\n",
    "                tgt_image, [H, W], method=tf.image.ResizeMethod.BILINEAR)\n",
    "            scaled_src_images_stack[s] = tf.image.resize(\n",
    "                src_image_stack, [H, W], method=tf.image.ResizeMethod.BILINEAR)\n",
    "    \n",
    "        curr_tgt_image = scaled_tgt_images[s]\n",
    "        curr_src_image_stack = scaled_src_images_stack[s]\n",
    "        curr_tgt_image_all.append(curr_tgt_image)\n",
    "        curr_src_image_stack_all.append(curr_src_image_stack)\n",
    "        \n",
    "        tgt_depth = depth['tgt'][s]\n",
    "        src0_depth = depth['src0'][s]\n",
    "        src1_depth = depth['src1'][s]\n",
    "        \n",
    "        \n",
    "        # src0\n",
    "        depth_flow_src02tgt, _ = inverse_warp(\n",
    "            src0_depth,\n",
    "            pred_poses[:, 0, 0:6], # src0 -> tgt (fw0)\n",
    "            proj_cam2pix[:, s, :, :],\n",
    "            proj_pix2cam[:, s, :, :])\n",
    "        curr_proj_image_tgt2src0 = transformer_old(curr_tgt_image, depth_flow_src02tgt, [H, W])\n",
    "        curr_proj_error_src0 = tf.abs(curr_proj_image_tgt2src0 - curr_src_image_stack[:,:,:,0:3])\n",
    "\n",
    "        depth_flow_src02src1, _ = inverse_warp(\n",
    "            src0_depth,\n",
    "            pred_poses[:, 0, 6:12], # src0 -> src1 (fw2)\n",
    "            proj_cam2pix[:, s, :, :],\n",
    "            proj_pix2cam[:, s, :, :])\n",
    "        curr_proj_image_src12src0 = transformer_old(curr_src_image_stack[:,:,:,3:6], depth_flow_src02src1, [H, W])\n",
    "        curr_proj_error_src0_1 = tf.abs(curr_proj_image_src12src0 - curr_src_image_stack[:,:,:,0:3])\n",
    "\n",
    "        # tgt\n",
    "        depth_flow_tgt2src1, _ = inverse_warp(\n",
    "            tgt_depth,\n",
    "            pred_poses[:, 0, 12:18], # tgt -> src1 (fw1)\n",
    "            proj_cam2pix[:, s, :, :],\n",
    "            proj_pix2cam[:, s, :, :])\n",
    "        curr_proj_image_src12tgt = transformer_old(curr_src_image_stack[:,:,:,3:6], depth_flow_tgt2src1, [H, W])\n",
    "        curr_proj_error_tgt = tf.abs(curr_proj_image_src12tgt - curr_tgt_image)\n",
    "\n",
    "        depth_flow_tgt2src0, _ = inverse_warp(\n",
    "            tgt_depth,\n",
    "            pred_poses[:, 0, 18:24], # tgt -> src0 (bw0)\n",
    "            proj_cam2pix[:, s, :, :],\n",
    "            proj_pix2cam[:, s, :, :])\n",
    "        curr_proj_image_src02tgt = transformer_old(curr_src_image_stack[:,:,:,0:3], depth_flow_tgt2src0, [H, W])\n",
    "        curr_proj_error_tgt_1 = tf.abs(curr_proj_image_src02tgt - curr_tgt_image)\n",
    "\n",
    "        # src1\n",
    "        depth_flow_src12src0, _ = inverse_warp(\n",
    "            src1_depth,\n",
    "            pred_poses[:, 0, 24:30], # src1 -> src0 (bw2)\n",
    "            proj_cam2pix[:, s, :, :],\n",
    "            proj_pix2cam[:, s, :, :])\n",
    "        curr_proj_image_src02src1 = transformer_old(curr_src_image_stack[:,:,:,0:3], depth_flow_src12src0, [H, W])\n",
    "        curr_proj_error_src1 = tf.abs(curr_proj_image_src02src1 - curr_src_image_stack[:,:,:,3:6])\n",
    "\n",
    "        depth_flow_src12tgt, _ = inverse_warp(\n",
    "            src1_depth,\n",
    "            pred_poses[:, 0, 30:36], # src1 -> tgt (bw1)\n",
    "            proj_cam2pix[:, s, :, :],\n",
    "            proj_pix2cam[:, s, :, :])\n",
    "        curr_proj_image_tgt2src1 = transformer_old(curr_tgt_image, depth_flow_src12tgt, [H, W])\n",
    "        curr_proj_error_src1_1 = tf.abs(curr_proj_image_tgt2src1 - curr_src_image_stack[:,:,:,3:6])\n",
    "        \n",
    "        \n",
    "        \n",
    "        if not compute_minimum_loss:\n",
    "            # src0\n",
    "            reconstructed_loss += tf.reduce_mean(input_tensor=curr_proj_error_src0 * occu_masks_bw[0][s]) / occu_masks_bw_avg[0][s]\n",
    "            cross_reconstructed_loss += tf.reduce_mean(input_tensor=curr_proj_error_src0_1 * occu_masks_bw[2][s]) / occu_masks_bw_avg[2][s]\n",
    "            # tgt\n",
    "            reconstructed_loss += tf.reduce_mean(input_tensor=curr_proj_error_tgt * occu_masks_bw[1][s]) / occu_masks_bw_avg[1][s]\n",
    "            reconstructed_loss += tf.reduce_mean(input_tensor=curr_proj_error_tgt_1 * occu_masks_fw[0][s]) / occu_masks_fw_avg[0][s]\n",
    "            # src1\n",
    "            cross_reconstructed_loss += tf.reduce_mean(input_tensor=curr_proj_error_src1 * occu_masks_fw[2][s]) / occu_masks_fw_avg[2][s]\n",
    "            reconstructed_loss += tf.reduce_mean(input_tensor=curr_proj_error_src1_1 * occu_masks_fw[1][s]) / occu_masks_fw_avg[1][s]\n",
    "\n",
    "            if ssim_weight > 0:\n",
    "                # src0\n",
    "                ssim_loss += tf.reduce_mean(input_tensor=SSIM(curr_proj_image_tgt2src0 * occu_masks_bw[0][s], curr_src_image_stack[:,:,:,0:3] * occu_masks_bw[0][s])) / occu_masks_bw_avg[0][s]\n",
    "                cross_ssim_loss += tf.reduce_mean(input_tensor=SSIM(curr_proj_image_src12src0 * occu_masks_bw[2][s], curr_src_image_stack[:,:,:,0:3] * occu_masks_bw[2][s])) / occu_masks_bw_avg[2][s]\n",
    "                # tgt\n",
    "                ssim_loss += tf.reduce_mean(input_tensor=SSIM(curr_proj_image_src12tgt * occu_masks_bw[1][s], curr_tgt_image * occu_masks_bw[1][s])) / occu_masks_bw_avg[1][s]\n",
    "                ssim_loss += tf.reduce_mean(input_tensor=SSIM(curr_proj_image_src02tgt * occu_masks_fw[0][s], curr_tgt_image * occu_masks_fw[0][s])) / occu_masks_fw_avg[0][s]\n",
    "                # src1\n",
    "                cross_ssim_loss += tf.reduce_mean(input_tensor=SSIM(curr_proj_image_src02src1 * occu_masks_fw[2][s], curr_src_image_stack[:,:,:,3:6] * occu_masks_fw[2][s])) / occu_masks_bw_avg[2][s]\n",
    "                ssim_loss += tf.reduce_mean(input_tensor=SSIM(curr_proj_image_tgt2src1 * occu_masks_fw[1][s], curr_src_image_stack[:,:,:,3:6] * occu_masks_fw[1][s])) / occu_masks_fw_avg[1][s]\n",
    "\n",
    "        if dp_smooth_weight > 0:\n",
    "            if depth_normalization:\n",
    "                # Perform depth normalization, dividing by the mean.\n",
    "                mean_tgt_disp = tf.reduce_mean(input_tensor=disp['tgt'][s], axis=[1, 2, 3], keepdims=True)\n",
    "                tgt_disp_input = disp['tgt'][s] / mean_tgt_disp\n",
    "                mean_src0_disp = tf.reduce_mean(input_tensor=disp['src0'][s], axis=[1, 2, 3], keepdims=True)\n",
    "                src0_disp_input = disp['src0'][s] / mean_src0_disp\n",
    "                mean_src1_disp = tf.reduce_mean(input_tensor=disp['src1'][s], axis=[1, 2, 3], keepdims=True)\n",
    "                src1_disp_input = disp['src1'][s] / mean_src1_disp\n",
    "            else:\n",
    "                tgt_disp_input = disp['tgt'][s]\n",
    "                src0_disp_input = disp['src0'][s]\n",
    "                src1_disp_input = disp['src1'][s]\n",
    "            scaling_f = (1.0 if equal_weighting else 1.0 / (2**s))\n",
    "            # Edge-aware first-order\n",
    "            smooth_loss += scaling_f * depth_smoothness(tgt_disp_input, scaled_tgt_images[s])\n",
    "            smooth_loss += scaling_f * depth_smoothness(src0_disp_input, scaled_src_images_stack[s][:,:,:,0:3])\n",
    "            smooth_loss += scaling_f * depth_smoothness(src1_disp_input, scaled_src_images_stack[s][:,:,:,3:6])\n",
    "    \n",
    "    # Loss Hyperparameters\n",
    "SSIM_WEIGHT = 0.85\n",
    "DP_RECONSTRUCTION_WEIGHT = 0.15\n",
    "DP_SMOOTH_WEIGHT = 0.1 # origin 1.0\n",
    "DP_CROSS_GEOMETRY_WEIGHT = 0.8\n",
    "    \n",
    "    reconstruct_losses = reconstructed_loss + DP_CROSS_GEOMETRY_WEIGHT * cross_reconstructed_loss\n",
    "    ssim_losses = ssim_loss + DP_CROSS_GEOMETRY_WEIGHT * cross_ssim_loss\n",
    "    losses =  DP_RECONSTRUCTION_WEIGHT * reconstruct_losses + \\\n",
    "              SSIM_WEIGHT * ssim_losses + \\\n",
    "              DP_SMOOTH_WEIGHT * smooth_loss\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def occulsion(pred_flow, H, W):\n",
    "    \"\"\"\n",
    "    Here, we compute the soft occlusion maps proposed in https://arxiv.org/pdf/1711.05890.pdf\n",
    "\n",
    "    pred_flow: the estimated forward optical flow\n",
    "    \"\"\"\n",
    "#     transformerFwd = TransformerFwd()\n",
    "    occu_mask = [\n",
    "        tf.clip_by_value(\n",
    "            transformerFwd(\n",
    "                tf.ones(shape=[BATCH_SIZE, H, W, 1], dtype='float32'),\n",
    "                pred_flow, [H , W], backprop=True),\n",
    "            clip_value_min=0.0,\n",
    "            clip_value_max=1.0)\n",
    "        ]\n",
    "    occu_mask = tf.reshape(occu_mask, [BATCH_SIZE, H, W, 1])\n",
    "    occu_mask_avg = tf.reduce_mean(input_tensor=occu_mask)\n",
    "\n",
    "    return occu_mask, occu_mask_avg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
